\section{Interpolants in Nonlinear Theories}
\label{sec:itp}

Our algorithm to construct a disjunctive linear interpolants takes as input a proof tree and a labelling functions.
Let $l$ be the labelling function that maps formula and variables to \textsc{a},\textsc{b}, or \textsc{ab}.
For each proof rule we associate an partial interpolant, written in square bracket on the right of the conclusion of the rule.

% need a labelling function
% ThLem: A → false 
%        B → true
% Split: A → I₁ ∨ I₂
%       AB → ite(x_i ≤ p, I₁, I₂) 
%        B → I₁ ∧ I₂
% Weakening: identify

\begin{mathpar}
\inferrule{ {} }{
  \vec x ∈ \vec D ∧ c \entails ⊥ \quad [l(f) ≠ \textsc{a}]
}{(\thLemI)}\\

\inferrule{
  C = c ∧ \bigwedge_k C_k \\
  \vec x ∈ \vec D ∧ c \entails ⊥ \quad [I]
}{
  \vec x ∈ \vec D ∧ C \entails ⊥ \quad [I]
}{(\weakenI)}\\


\inferrule{
  x_i ∈ [l_i, p] ∧ \bigwedge_{j ≠ i} x_j ∈ D_j ∧ C \entails ⊥ \quad [I₁] \\\\
  x_i ∈ [p, u_i] ∧ \bigwedge_{j ≠ i} x_j ∈ D_j ∧ C \entails ⊥ \quad [I₂] 
}{
  x_i∈ [l_i, u_i]\wedge \bigwedge_{j\neq i} x_j ∈ D_
  j ∧ C \entails ⊥ \quad
  \left[ \substack{ I₁ ∨ I₂     \qquad \quad ~~  \text{if} ~ l(x_i) = \textsc{a} \\
                    ite(x_i < p, I₁, I₂) ~~ \text{if} ~ l(x_i) = \textsc{ab}\\
                    I₁ ∧ I₂     \qquad \quad ~~  \text{if} ~ l(x_i) = \textsc{b}}\right]
}{(\spltI)}

\end{mathpar}

where $ite(x,y,z)$ is a shorthand for $(x ∧ y)∨(¬x ∧ z)$

\todo[inline]{find a better way to format that.}

Intuitively, a proof of unsatisfiability is a tiling of the solution space where each tile is associated with a conjunct $f$ from $A ∧ B$.
$f$ is a witness that shows the absence of solution in a given tile.
The interpolation rules traverse the rules and selects which tiles belong to the interpolant $I$.

At the leaf level (rule \thLemI), the tile is in $I$ if $f$ is not part of $A$, i.e., the contradiction originates from $B$.
If $f$ is in both $A$ and $B$ then it can be considered as either part of $A$ or $B$.
Both cases leads to a correct interpolant.
The \weakenI rule does not influence the interpolant, it is only required to pick $f$ from $A ∧ B$.

The \spltI is the most interesting rule.
Splitting the domains essentially defined the bounds of the subsequent tiles.
Let $x$ be the variable whose domain is split at value $p$ and $I₁$, $I₂$ be the two interpolants for the case when $x < p$ and $x ≥ p$.
If $x$ occurs in $A$ but not $B$, then $x$ cannot occur in $I$.
Since $x$ is in $A$ then we know that $A$ implies $x < p ⇒ I₁$ and $x ≥ p ⇒ I₂$.
Eliminating $x$ give $I = I₁ ∨ I₂$.
A similar reasoning is applicable when $x$ occurs in $B$ but not $A$ and gives $I = I₁ ∧ I₂$.
When $x$ occurs in both $A$ and $B$ then $x$ is kept in $I$ and acts as a selector for the values of $x$ smaller than $p$ $I₁$ is selected, otherwise $I₂$ applies.

The correctness of our method is shown by the following theorem:
\begin{theorem}
The rules \spltI, \thLemI, \weakenI generate a Craig interpolant $I$ from the proof of unsatisfiability of $A$ and $B$.
\label{thm:sound}
\end{theorem}
\begin{proof}
%\emph{(Sketch)}
We prove correctness of the rules by induction.
To express the inductive invariant, we split the domain $\vec D$ into the domains $\vec D_A$ and $\vec D_B$ which contains only the intervals of the variables occuring in $A$, $B$ respectively.

At any given point in the proof, at any given point in the proof the partial interpolant $I$ is an interpolant for the formula $A$ over $\vec D_A$ and $B$ over $\vec D_B$.
At the root of the proof tree we get an interpolant for the whole domain $\vec D = \vec D_A ∧ \vec D_B$.

At the leaves of the proof, or the \thLemI rule, one of the constrain has no solution over the domain.
Let's assume that this constraint comes from $A$.
Then the partial interpolant $I$ is $⊥$.
We have that $A ∧ \vec D_A ⇒ I$ by the semantics of the \thLem rule ($⊥⇒⊥$).
Trivially $B ∧ \vec D_B ∧ I ⇒ ⊥$ and $fv(I) = ∅ ⊆ fv(A) ∩ fv(B)$.
When the contradiction comes from $B$, a similar reasoning is applied with $I=⊤$.

The \weakenI only serves to select which constraint cause the contradiction and does not changed the invariant.

The \spltI rule is the most complex case.
We have to consider whether the variable $x$ which is split come from $A$, $B$, or is shared.
For instance, if $x∈ fv(A)$ then the induction step has $\vec D_{A1} = \vec D_A ∧ x < p$ and $\vec D_{A2} = \vec D_A ∧ x ≥ p$ and $\vec D_B$ is unchanged.
if $x ∈ fv(B)$ then $\vec D_B$ is affected and $\vec D_A$ is unchanged.
if $x$ is shared then both $\vec D_A$ and $\vec D_B$ are affected.

Let consider that $x ∈ fv(A)$ and $x ∉ fv(B)$.
We omit the case where $x$ is in $B$ but not $A$ as it is similar.
The induction hypothesis is\\
\parbox{0.35\linewidth}{
\begin{eqnarray*}
& A ∧ (\vec D_A ∧ x < p) ⇒ I₁ \\
& A ∧ (\vec D_A ∧ x ≥ p) ⇒ I₂ \\
& B ∧ \vec D_B ∧ I₁ ⇒ ⊥ \\
& B ∧ \vec D_B ∧ I₂ ⇒ ⊥
\end{eqnarray*}
}
which simplifies to
\parbox{0.35\linewidth}{
\begin{eqnarray*}
& A ∧ \vec D_A ⇒ I₁ ∨ I₂ \\
& B ∧ \vec D_B ∧ (I₁ ∨ I₂) ⇒ ⊥
\end{eqnarray*}
}.

Finally, we need to consider $x ∈ fv(A)$ and $x ∈ fv(B)$.
The induction hypothesis is\\
\parbox{0.38\linewidth}{
\begin{eqnarray*}
& A ∧ (\vec D_A ∧ x < p) ⇒ I₁ \\
& A ∧ (\vec D_A ∧ x ≥ p) ⇒ I₂ \\
& B ∧ (\vec D_B ∧ x < p) ∧ I₁ ⇒ ⊥ \\
& B ∧ (\vec D_B ∧ x ≥ p) ∧ I₂ ⇒ ⊥
\end{eqnarray*}
}
which simplifies to
\parbox{0.42\linewidth}{
\begin{eqnarray*}
& A ∧ \vec D_A ⇒ ite(x < p, I₁, I₂)\\
& B ∧ \vec D_B ∧ ite(x < p, I₁, I₂) ⇒ ⊥
\end{eqnarray*}
}.
\qed
\end{proof}

\begin{example}
If we look at proof for the example in Figure~\ref{fig:example}, we get the following proof shown in Figure~\ref{fig:proof}.
The proof is also annotated with the partial interpolants.
The final interpolants $I₄$ is $0≤y ∧ (0.26≤y ∨ (y≤0.26 ∧ -0.51 ≤ x ≤ 0.51))$.

\begin{figure}
\centering
\begin{tikzpicture}
\node (box){%
  \begin{minipage}{\textwidth}
{\small
\begin{mathpar}
\inferrule{
    \inferrule{ {} }{
        x ∈ [-0.51,0.51] ∧ y ∈ [0,0.26] ∧ B \entails ⊥ \quad [ ⊤ ]
    }{(\thLemI)}
}{
    x ∈ [-0.51,0.51] ∧ y ∈ [0,0.26] ∧ A ∧ B \entails ⊥ \quad [ I₁: ⊤ ] \\\\ \vdots
}{(\weakenI)}

\inferrule{
    \inferrule{
        \inferrule{ {} }{
            x ∈ [0.51,1] ∧ y ∈ [0,0.26] ∧ A \entails ⊥ \quad [ ⊥ ]
        }{(\thLemI)}
    }{
        x ∈ [0.51,1] ∧ y ∈ [0,0.26] ∧ A ∧ B \entails ⊥ \quad [ ⊥ ]
    }{(\weakenI)} \\
    \vdots ~~ [ I₁ ]
}{
  x ∈ [-0.51,1] ∧ y ∈ [0,0.26] ∧ a ∧ b \entails \bot \quad [ I₂: x≤0.51 ] \\\\ \vdots
}{(\spltI)}

\inferrule{
    \vdots ~~ [ I₂ ] \\
    \inferrule{
        \inferrule{ {} }{
            x ∈ [-1,-0.51] ∧ y ∈ [0,0.26] ∧ A \entails ⊥ \quad [ ⊥ ]
        }{(\thLemI)}
    }{
        x ∈ [-1,-0.51] ∧ y ∈ [0,0.26] ∧ A ∧ B \entails ⊥ \quad [ ⊥ ]
    }{(\weakenI)}
}{
  x ∈ [-1,1] ∧ y ∈ [0,0.26] ∧ a ∧ b \entails \bot \quad [ I₃: -0.51≤x≤0.51 ] \\\\ \vdots
}
        
\inferrule{
    \vdots ~~ [ I₃ ] \\
    \inferrule{
        \inferrule{
            {}
        }{
            x ∈ [-1,1] ∧ y ∈ [0.26,1] ∧ B \entails \bot \quad [ \top ]
        }{(\thLemI)}
    }{
        x ∈ [-1,1] ∧ y ∈ [0.26,1] ∧ A ∧ B \entails \bot \quad [ \top ]
    }{(\weakenI)}
}{
    x ∈ [-1,1] ∧ y ∈ [0,1] ∧ A ∧ B \entails \bot \quad [ I₄: ~ 0.26 \leq y \lor (y \leq 0.26 ∧ I₃) ] \\\\ \vdots
}{(\spltI)}

\inferrule{
    \inferrule{
        \inferrule{
            {}
        }{
            x ∈ [-1,1] ∧ y ∈ [-1,0] ∧ A \entails \bot \quad [ \bot ]
        }{(\thLemI)}
    }{
        x ∈ [-1,1] ∧ y ∈ [-1,0] ∧ A ∧ B \entails \bot \quad [ \bot ]
    }{(\weakenI)} \\
    \vdots ~~ [ I₄ ]
}{
    x ∈ [-1,1] ∧ y ∈ [-1,1] ∧ A ∧ B \entails \bot \quad [ 0 \leq y ∧ I₄ ]
}{(\spltI)}
\end{mathpar}
}
  \end{minipage}
};
\draw [loosely dotted,thick] plot [smooth,tension=1] coordinates { (-0.95,4.4) (-0.0,4) (3.2,4.0) (4.1,3.4) } ;
\draw [loosely dotted,thick] plot [smooth,tension=1] coordinates { (-0.55,1.5) (-1.5,1.2) (-4.2,1.2) (-5.63,0.6) } ;
\draw [loosely dotted,thick] plot [smooth,tension=1] coordinates { (-0.0,-1.35) (-1.2,-1.8) (-4.5,-1.8) (-5.85,-2.3) } ;
\draw [loosely dotted,thick] plot [smooth,tension=1] coordinates { (-0.55,-4.2) (0.3,-4.6) (3.0,-4.6) (3.96,-5.15) } ;
\end{tikzpicture}
\caption{Proof for $y≥x² ∧ y ≤ -\cos(x) + 0.8$ and corresponding interpolant.}
\label{fig:proof}
\end{figure}
\end{example}

\paragraph{Boolean structure.}
The method we presented explain how to compute an interpolant for the conjunctive fragment of quantififer-free nonlinear theories over ℝ.
However, in many cases formula also contains disjunctions.
To handle disjunctions, our method can be combined with the method presented by Yorsh and Musuvathi~\cite{DBLP:conf/cade/YorshM05} for building an interpolant from a resolution proof where some of the proof's leaves are interpolant for specific theory.

\subsection{Interpolant Strength}

When using interpolation as heuristic for accelerating fixed point computation for verification, it is difficult to know a priori what is a good interpolant.
In such application, it is desirable to have the possibility of generating and trying multiple interpolants.
We now explain how to get multiple interpolant of different logical strength.

\paragraph{δ-interpolants.}
The interpolation method that we propose uses a δ-decision procedure to build a Craig interpolant.
The properties of the interpolant means that $A ∧ ¬I$ and $B ∧ I$ are both unsatisfiable.
However, they are not necessarily δ-unsatisfiable.

To obtain an interpolant such that both $A ∧ ¬I$ and $B ∧ I$ are δ-unsatisfiable, we can weaken both $A$ and $B$ by a factor δ.
However, $A$ and $B$ must be at least $3δ$-unsatisfiable to guarantee that the solver finds a proof of unsatisfiability.
Furthermore, we can also introduce perturbation only on one side in other to make the interpolant stronger of weaker.

\paragraph{Changing the labelling.}
Do to the similarity of our method to interpolation of propositional formula we can borrow result about adapting the labelled interpolation system from D'Silva et.al.~\cite{DBLP:conf/vmcai/DSilvaKPW10} to our framework.

In the labelled interpolation system, it is possible to modify the \textsc{a,b,ab} labelling as long as it preserves \emph{locality}.
The restriction over D'Silva et.al.~system is that we cannot use projection for the leaves of the proof as this projection is not computable to nonlinear theories.
Therefore, the labelling must still enforce that the leaves maps to the partial interpolants ⊤ or ⊥.
