\section{Interpolants in Nonlinear Theories}
\label{sec:itp}

Intuitively, a proof of unsatisfiability is a partition of the solution space where each sub-domain is associated with a conjunct $c$ from $A ∧ B$.
$c$ is a witness that shows the absence of solution in a given domain.
The interpolation rules traverse the rules and selects which parts belong to the interpolant $I$. We now describe the algorithm for obtaining such interpolants for formulas $A$ and $B$ from the proof of unsatisfiability for $A\wedge B$. 

% need a labelling function
% ThLem: A → false 
%        B → true
% Split: A → I₁ ∨ I₂
%       AB → ite(x_i ≤ p, I₁, I₂) 
%        B → I₁ ∧ I₂
% Weakening: identify

\subsection{Core Algorithms}

\begin{figure}
\centering
\begin{mathpar}
\inferrule{ {} }{
  \vec x ∈ \vec D ∧ c \entails ⊥ \quad [l(c) ≠ \textsc{a}]
}{(\thLemI)}\\

\inferrule{
  C = c ∧ \bigwedge_k C_k \\
  \vec x ∈ \vec D ∧ c \entails ⊥ \quad [I]
}{
  \vec x ∈ \vec D ∧ C \entails ⊥ \quad [I]
}{(\weakenI)}\\


\inferrule{
  x_i ∈ [l_i, p] ∧ \bigwedge_{j ≠ i} x_j ∈ D_j ∧ C \entails ⊥ \quad [I₁] \\\\
  x_i ∈ [p, u_i] ∧ \bigwedge_{j ≠ i} x_j ∈ D_j ∧ C \entails ⊥ \quad [I₂] 
}{
  x_i∈ [l_i, u_i]\wedge \bigwedge_{j\neq i} x_j ∈ D_
  j ∧ C \entails ⊥ \quad
  \left[{\large \substack{ I₁ ∨ I₂     \qquad \quad ~~  \text{if} ~ l(x_i) = \textsc{a} \\
                    ite(x_i < p, I₁, I₂) ~~ \text{if} ~ l(x_i) = \textsc{ab}\\
                    I₁ ∧ I₂     \qquad \quad ~~  \text{if} ~ l(x_i) = \textsc{b}} }\right ]
}{(\spltI)}

\end{mathpar}
where $ite(x,y,z)$ is a shorthand for $(x ∧ y)∨(¬x ∧ z)$
\caption{Interpolant producing proof rules}
\label{fig:rulesI}
\end{figure}

Our method for constructing disjunctive linear interpolants takes two inputs: a proof tree and a labeling function. The labeling function maps formula and variables to either \textsc{a}, \textsc{b}, or \textsc{ab}.
For each proof rule introduced in Figure~\ref{fig:rules}, we associate some partial interpolants, written in square bracket on the right of the conclusion of the rule. 
Figure~\ref{fig:rulesI} shows these modified versions of the rules.
\begin{itemize}
\item At the leaf level (rule \thLemI), the tile is in $I$ if $c$ is not part of $A$, i.e., the contradiction originates from $B$.
If $c$ is in both $A$ and $B$ then it can be considered as either part of $A$ or $B$.
Both cases lead to a correct interpolant.

\item The \weakenI rule does not influence the interpolant, it is only required to pick $c$ from $A ∧ B$.

\item The \spltI is the most interesting rule.
Splitting the domain essentially defines the bounds of the subsequent domains.
Let $x$ be the variable whose domain is split at value $p$ and $I₁$, $I₂$ be the two interpolants for the case when $x < p$ and $x ≥ p$.
If $x$ occurs in $A$ but not $B$, then $x$ cannot occur in $I$.
Since $x$ is in $A$ then we know that $A$ implies $x < p ⇒ I₁$ and $x ≥ p ⇒ I₂$.
Eliminating $x$ gives $I = I₁ ∨ I₂$.
A similar reasoning applies when $x$ occurs in $B$ but not $A$ and gives $I = I₁ ∧ I₂$.
When $x$ occurs in both $A$ and $B$ then $x$ is kept in $I$ and acts as a selector for the values of $x$ smaller than $p$ $I₁$ is selected, otherwise $I₂$ applies.
\end{itemize}
The correctness of our method is shown by the following theorem:
\begin{theorem}
The rules \spltI, \thLemI, \weakenI generate a Craig interpolant $I$ from the proof of unsatisfiability of $A$ and $B$.
\label{thm:sound}
\end{theorem}
\begin{proof}
%\emph{(Sketch)}
We prove correctness of the rules by induction.
To express the inductive invariant, we split the domain $\vec D$ into the domains $\vec D_A$ and $\vec D_B$ which contains only the intervals of the variables occuring in $A$, $B$ respectively.

At any given point in the proof, the partial interpolant $I$ is an interpolant for the formula $A$ over $\vec D_A$ and $B$ over $\vec D_B$.
At the root of the proof tree we get an interpolant for the whole domain $\vec D = \vec D_A ∧ \vec D_B$.

At the leaves of the proof, or the \thLemI rule, one of the constraints has no solution over the domain.
Let's assume that this constraint comes from $A$.
Then the partial interpolant $I$ is $⊥$.
We have that $A ∧ \vec D_A ⇒ I$ by the semantics of the \thLem rule ($⊥⇒⊥$).
Trivially, $B ∧ \vec D_B ∧ I ⇒ ⊥$ and $fv(I) = ∅ ⊆ fv(A) ∩ fv(B)$.
When the contradiction comes from $B$, a similar reasoning applies with $I=⊤$.

The \weakenI only serves to select the constraint which causes the contradiction and does not change the invariant.

The \spltI rule is the most complex case.
We have to consider whether the variable $x$ which is split come from $A$, $B$, or is shared.
For instance, if $x∈ fv(A)$ then the induction step has $\vec D_{A1} = \vec D_A ∧ x < p$ and $\vec D_{A2} = \vec D_A ∧ x ≥ p$ and $\vec D_B$ is unchanged.
If $x ∈ fv(B)$ then $\vec D_B$ is affected and $\vec D_A$ is unchanged.
If $x$ is shared then both $\vec D_A$ and $\vec D_B$ are affected.

Let consider that $x ∈ fv(A)$ and $x ∉ fv(B)$.
We omit the case where $x$ is in $B$ but not $A$ as it is similar.
The induction hypothesis is\\
\parbox{0.35\linewidth}{
\begin{eqnarray*}
& A ∧ (\vec D_A ∧ x < p) ⇒ I₁ \\
& A ∧ (\vec D_A ∧ x ≥ p) ⇒ I₂ \\
& B ∧ \vec D_B ∧ I₁ ⇒ ⊥ \\
& B ∧ \vec D_B ∧ I₂ ⇒ ⊥
\end{eqnarray*}
}
which simplifies to
\parbox{0.35\linewidth}{
\begin{eqnarray*}
& A ∧ \vec D_A ⇒ I₁ ∨ I₂ \\
& B ∧ \vec D_B ∧ (I₁ ∨ I₂) ⇒ ⊥
\end{eqnarray*}
}.

Finally, we need to consider $x ∈ fv(A)$ and $x ∈ fv(B)$.
The induction hypothesis is\\
\parbox{0.38\linewidth}{
\begin{eqnarray*}
& A ∧ (\vec D_A ∧ x < p) ⇒ I₁ \\
& A ∧ (\vec D_A ∧ x ≥ p) ⇒ I₂ \\
& B ∧ (\vec D_B ∧ x < p) ∧ I₁ ⇒ ⊥ \\
& B ∧ (\vec D_B ∧ x ≥ p) ∧ I₂ ⇒ ⊥
\end{eqnarray*}
}
which simplifies to
\parbox{0.42\linewidth}{
\begin{eqnarray*}
& A ∧ \vec D_A ⇒ ite(x < p, I₁, I₂)\\
& B ∧ \vec D_B ∧ ite(x < p, I₁, I₂) ⇒ ⊥
\end{eqnarray*}
}.
\qed
\end{proof}

\begin{example}
If we look at proof for the example in Figure~\ref{fig:example}, we get the proof annotated with the partial interpolants shown in Figure~\ref{fig:proof}.
The final interpolants $I₅$ is $0≤y ∧ (0.26≤y ∨ (y≤0.26 ∧ -0.51 ≤ x ≤ 0.51))$.

\begin{figure}
\centering
\begin{tikzpicture}
\node (box){%
  \begin{minipage}{\textwidth}
{\small
\begin{mathpar}
\inferrule{
    \inferrule{ {} }{
        x ∈ [-0.51,0.51] ∧ y ∈ [0,0.26] ∧ B \entails ⊥ \quad [ ⊤ ]
    }{(\thLemI)}
}{
    x ∈ [-0.51,0.51] ∧ y ∈ [0,0.26] ∧ A ∧ B \entails ⊥ \quad [ I₁: ⊤ ] \\\\ \vdots
}{(\weakenI)}

\inferrule{
    \inferrule{
        \inferrule{ {} }{
            x ∈ [0.51,1] ∧ y ∈ [0,0.26] ∧ A \entails ⊥ \quad [ ⊥ ]
        }{(\thLemI)}
    }{
        x ∈ [0.51,1] ∧ y ∈ [0,0.26] ∧ A ∧ B \entails ⊥ \quad [ ⊥ ]
    }{(\weakenI)} \\
    \vdots ~~ [ I₁ ]
}{
  x ∈ [-0.51,1] ∧ y ∈ [0,0.26] ∧ a ∧ b \entails \bot \quad [ I₂: x≤0.51 ] \\\\ \vdots
}{(\spltI)}

\inferrule{
    \vdots ~~ [ I₂ ] \\
    \inferrule{
        \inferrule{ {} }{
            x ∈ [-1,-0.51] ∧ y ∈ [0,0.26] ∧ A \entails ⊥ \quad [ ⊥ ]
        }{(\thLemI)}
    }{
        x ∈ [-1,-0.51] ∧ y ∈ [0,0.26] ∧ A ∧ B \entails ⊥ \quad [ ⊥ ]
    }{(\weakenI)}
}{
  x ∈ [-1,1] ∧ y ∈ [0,0.26] ∧ a ∧ b \entails \bot \quad [ I₃: -0.51≤x≤0.51 ] \\\\ \vdots
}
        
\inferrule{
    \vdots ~~ [ I₃ ] \\
    \inferrule{
        \inferrule{
            {}
        }{
            x ∈ [-1,1] ∧ y ∈ [0.26,1] ∧ B \entails \bot \quad [ \top ]
        }{(\thLemI)}
    }{
        x ∈ [-1,1] ∧ y ∈ [0.26,1] ∧ A ∧ B \entails \bot \quad [ \top ]
    }{(\weakenI)}
}{
    x ∈ [-1,1] ∧ y ∈ [0,1] ∧ A ∧ B \entails \bot \quad [ I₄: ~ 0.26 \leq y \lor (y \leq 0.26 ∧ I₃) ] \\\\ \vdots
}{(\spltI)}

\inferrule{
    \inferrule{
        \inferrule{
            {}
        }{
            x ∈ [-1,1] ∧ y ∈ [-1,0] ∧ A \entails \bot \quad [ \bot ]
        }{(\thLemI)}
    }{
        x ∈ [-1,1] ∧ y ∈ [-1,0] ∧ A ∧ B \entails \bot \quad [ \bot ]
    }{(\weakenI)} \\
    \vdots ~~ [ I₄ ]
}{
    x ∈ [-1,1] ∧ y ∈ [-1,1] ∧ A ∧ B \entails \bot \quad [ I₅: ~ 0 \leq y ∧ I₄ ]
}{(\spltI)}
\end{mathpar}
}
  \end{minipage}
};
\draw [loosely dotted,thick] plot [smooth,tension=1] coordinates { (-0.95,4.4) (-0.0,4) (3.2,4.0) (4.1,3.4) } ;
\draw [loosely dotted,thick] plot [smooth,tension=1] coordinates { (-0.55,1.5) (-1.5,1.2) (-4.2,1.2) (-5.63,0.6) } ;
\draw [loosely dotted,thick] plot [smooth,tension=1] coordinates { (-0.0,-1.35) (-1.2,-1.8) (-4.5,-1.8) (-5.85,-2.3) } ;
\draw [loosely dotted,thick] plot [smooth,tension=1] coordinates { (-0.55,-4.2) (0.3,-4.6) (3.0,-4.6) (3.96,-5.15) } ;
\end{tikzpicture}
\caption{Proof of unsatisfiability where $A$ is $y≥x²$, $B$ is $y ≤ -\cos(x) + 0.8$ along with the corresponding interpolant}
\label{fig:proof}
\end{figure}
\end{example}

\paragraph{Boolean structure.}
The method we presented explain how to compute an interpolant for the conjunctive fragment of quantifier-free nonlinear theories over the reals.
However, in many cases formula also contains disjunctions.
To handle disjunctions, our method can be combined with the method presented by Yorsh and Musuvathi~\cite{DBLP:conf/cade/YorshM05} for building an interpolant from a resolution proof where some of the proof's leaves carry theory interpolants.

\input{odes.tex}


\subsection{Extensions}

%When using interpolation as an heuristic to accelerate fixed point computation for verification,
For any two formulas $A$,$B$ which conjunction is unsatisfiable, the interpolant $I$ is not unique.
In practice, it is difficult to know a priori what is a good interpolant.
Therefore, it is desirable to have the possibility of generating and testing multiple interpolants.
We now explain how to get multiple interpolant of different logical strength.

\paragraph{Parameterizing interpolation strength.}

The interpolation method that we propose uses a δ-decision procedure to build a Craig interpolant.
$I$ being an interpolant means that $A ∧ ¬I$ and $B ∧ I$ are both unsatisfiable.
However, these formulas might still be δ-satisfiable.

To obtain an interpolant such that both $A ∧ ¬I$ and $B ∧ I$ are δ-unsatisfiable, we can weaken both $A$ and $B$ by a factor δ.
However, $A$ and $B$ must be at least $3δ$-unsatisfiable to guarantee that the solver finds a proof of unsatisfiability.
Furthermore, we can also introduce perturbations only on one side in other to make the interpolant stronger of weaker.
To introduce a perturbation $\xi$, we apply the following rewriting to every inequalities in $A$ and/or $B$: \todo{δ vs $\xi$}
\begin{eqnarray*}
L = R & ~~ \mapsto ~~ & L ≥ R - \xi ∧ L ≤ R + \xi \\
L ≥ R & ~~ \mapsto ~~ & L ≥ R - \xi \\
L > R & ~~ \mapsto ~~ & L > R - \xi \\
\end{eqnarray*}

\paragraph{Changing the labelling.}
\todo{this paragraph could use a better explanation}
Due to the similarity of our method to the interpolation of propositional formulas we can adapt the labelled interpolation system from D'Silva et.al.~\cite{DBLP:conf/vmcai/DSilvaKPW10} to our framework.

In the labelled interpolation system, it is possible to modify the \textsc{a,b,ab} labelling as long as it preserves \emph{locality}, see \cite{DBLP:conf/vmcai/DSilvaKPW10} for the details.
An additional restriction in our case is that we cannot use a projection of constraints at the proof's leaves.
The projection is not computable in nonlinear theories.
Therefore, the labelling must enforce that the leaves maps to the partial interpolants ⊤ or ⊥.
